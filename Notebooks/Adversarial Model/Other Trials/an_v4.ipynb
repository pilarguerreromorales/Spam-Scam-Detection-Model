{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11499795,
          "sourceType": "datasetVersion",
          "datasetId": 7209359
        },
        {
          "sourceId": 11499803,
          "sourceType": "datasetVersion",
          "datasetId": 7209367
        },
        {
          "sourceId": 11526880,
          "sourceType": "datasetVersion",
          "datasetId": 7229398
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Network Trial 4 - Logistic Regression and uni and bigram TF-IDF es Detector\n",
        "**Authors:** Matías Arévalo, Pilar Guerrero, Moritz Goebbels, Tomás Lock, Allan Stalker  \n",
        "**Date:** January – May 2025  "
      ],
      "metadata": {
        "id": "z59t1hPHubN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Download and Setup"
      ],
      "metadata": {
        "id": "iYw03kavubN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:08:20.508721Z",
          "iopub.execute_input": "2025-04-23T10:08:20.509430Z",
          "iopub.status.idle": "2025-04-23T10:09:55.115956Z",
          "shell.execute_reply.started": "2025-04-23T10:08:20.509407Z",
          "shell.execute_reply": "2025-04-23T10:09:55.114901Z"
        },
        "id": "csBSFn2DHzXm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade unsloth\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:03.175645Z",
          "iopub.execute_input": "2025-04-23T10:10:03.176722Z",
          "iopub.status.idle": "2025-04-23T10:10:17.415886Z",
          "shell.execute_reply.started": "2025-04-23T10:10:03.176694Z",
          "shell.execute_reply": "2025-04-23T10:10:17.415062Z"
        },
        "id": "wwOSU04xHzXn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:23.642782Z",
          "iopub.execute_input": "2025-04-23T10:10:23.643111Z",
          "iopub.status.idle": "2025-04-23T10:10:27.609258Z",
          "shell.execute_reply.started": "2025-04-23T10:10:23.643073Z",
          "shell.execute_reply": "2025-04-23T10:10:27.608350Z"
        },
        "id": "Ve0_fCY3HzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:32.652518Z",
          "iopub.execute_input": "2025-04-23T10:10:32.652950Z",
          "iopub.status.idle": "2025-04-23T10:10:32.781609Z",
          "shell.execute_reply.started": "2025-04-23T10:10:32.652914Z",
          "shell.execute_reply": "2025-04-23T10:10:32.780884Z"
        },
        "id": "xNVz39m7HzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! Llama 3 is up to 8k\n",
        "dtype = None\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,   # ✅ only this\n",
        "    dtype = None\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:39.205853Z",
          "iopub.execute_input": "2025-04-23T10:10:39.206409Z",
          "iopub.status.idle": "2025-04-23T10:11:33.843178Z",
          "shell.execute_reply.started": "2025-04-23T10:10:39.206388Z",
          "shell.execute_reply": "2025-04-23T10:11:33.842596Z"
        },
        "id": "FdA_DClLHzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:11:38.163985Z",
          "iopub.execute_input": "2025-04-23T10:11:38.164638Z",
          "iopub.status.idle": "2025-04-23T10:11:45.223426Z",
          "shell.execute_reply.started": "2025-04-23T10:11:38.164608Z",
          "shell.execute_reply": "2025-04-23T10:11:45.222801Z"
        },
        "id": "_Osh91ISHzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "13FoDmQOutcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(\n",
        "    \"/content/drive/MyDrive/Codes/AI Project v2/First_Checkpoint/checkpoint-100\"\n",
        ",\n",
        "    adapter_name=\"default\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:11:47.224564Z",
          "iopub.execute_input": "2025-04-23T10:11:47.225283Z",
          "iopub.status.idle": "2025-04-23T10:11:49.645570Z",
          "shell.execute_reply.started": "2025-04-23T10:11:47.225244Z",
          "shell.execute_reply": "2025-04-23T10:11:49.644727Z"
        },
        "id": "3Aty9uDUHzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:16:52.787268Z",
          "iopub.execute_input": "2025-04-23T10:16:52.787987Z",
          "iopub.status.idle": "2025-04-23T10:16:52.791970Z",
          "shell.execute_reply.started": "2025-04-23T10:16:52.787963Z",
          "shell.execute_reply": "2025-04-23T10:16:52.791254Z"
        },
        "id": "-NgdaWcwubN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Codes/AI Project v2/naivesbayesdata/train_generated.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:16:50.719882Z",
          "iopub.execute_input": "2025-04-23T10:16:50.720421Z",
          "iopub.status.idle": "2025-04-23T10:16:51.028462Z",
          "shell.execute_reply.started": "2025-04-23T10:16:50.720395Z",
          "shell.execute_reply": "2025-04-23T10:16:51.027838Z"
        },
        "id": "i1ZY1wNtubN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract X/y from your pre‑split DataFrames\n",
        "X_train = train['clean_message'].dropna()\n",
        "y_train = train['label'].loc[X_train.index]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:16:54.857198Z",
          "iopub.execute_input": "2025-04-23T10:16:54.857796Z",
          "iopub.status.idle": "2025-04-23T10:16:54.868497Z",
          "shell.execute_reply.started": "2025-04-23T10:16:54.857770Z",
          "shell.execute_reply": "2025-04-23T10:16:54.867526Z"
        },
        "id": "sgEaKxxzubN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_lr = Pipeline([\n",
        "    ('vect', CountVectorizer(\n",
        "        analyzer='word',\n",
        "        ngram_range=(1,2),\n",
        "        lowercase=True\n",
        "    )),\n",
        "    ('clf', LogisticRegression(\n",
        "        solver='liblinear',\n",
        "        C=1.0,\n",
        "        class_weight='balanced'  # optional\n",
        "    ))\n",
        "])\n",
        "\n",
        "baseline_lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:17:10.799331Z",
          "iopub.execute_input": "2025-04-23T10:17:10.799741Z",
          "iopub.status.idle": "2025-04-23T10:17:11.699944Z",
          "shell.execute_reply.started": "2025-04-23T10:17:10.799716Z",
          "shell.execute_reply": "2025-04-23T10:17:11.699210Z"
        },
        "id": "JN_e7mNlHzXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load full merged dataset (for ham if needed later)\n",
        "merged_df = pd.read_csv(\"/content/drive/MyDrive/Codes/AI Project v2/dataset/merged_data (1).csv\")\n",
        "EOS_TOKEN = tokenizer.eos_token or \"\"\n",
        "\n",
        "# --- 🧠 Spam-Only Dataset ---\n",
        "spam_df = pd.read_csv(\"/content/drive/MyDrive/Codes/AI Project v2/dataset/final_scam_prompt_sample (3).csv\")\n",
        "spam_df = spam_df.drop(columns=[\"label\"])\n",
        "spam_df.rename(columns={\"clean_message\": \"completion\"}, inplace=True)\n",
        "spam_df[\"full_text\"] = spam_df[\"prompt\"] + \"\\n\\n\" + spam_df[\"completion\"] + EOS_TOKEN\n",
        "spam_df[\"is_spam\"] = 1\n",
        "\n",
        "# ✅ Shuffle the spam dataset\n",
        "spam_df = spam_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# --- 🧠 Tokenize ---\n",
        "tokenized = tokenizer(\n",
        "    spam_df[\"full_text\"].tolist(),\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_seq_length,\n",
        "    return_tensors=\"np\",\n",
        ")\n",
        "\n",
        "# --- 📦 HuggingFace Dataset ---\n",
        "final_data = {\n",
        "    \"input_ids\": tokenized[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "    \"labels\": tokenized[\"input_ids\"].copy(),\n",
        "    \"is_spam\": spam_df[\"is_spam\"].tolist(),\n",
        "    \"prompt\": spam_df[\"prompt\"].tolist(),\n",
        "}\n",
        "dataset = Dataset.from_dict(final_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WeZv3ELzHzXp",
        "execution": {
          "iopub.status.busy": "2025-04-23T10:12:52.677661Z",
          "iopub.execute_input": "2025-04-23T10:12:52.678051Z",
          "iopub.status.idle": "2025-04-23T10:13:01.927283Z",
          "shell.execute_reply.started": "2025-04-23T10:12:52.678018Z",
          "shell.execute_reply": "2025-04-23T10:13:01.926619Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "spam_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:13:07.267541Z",
          "iopub.execute_input": "2025-04-23T10:13:07.268421Z",
          "iopub.status.idle": "2025-04-23T10:13:07.292449Z",
          "shell.execute_reply.started": "2025-04-23T10:13:07.268393Z",
          "shell.execute_reply": "2025-04-23T10:13:07.291453Z"
        },
        "id": "Pxmh6wGv665C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "class AdversarialTrainer(Trainer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.losses = []\n",
        "    self.steps = []\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "    is_spam = inputs.pop(\"is_spam\", None)\n",
        "    prompts = inputs.pop(\"prompt\", None)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    base_loss = outputs.loss\n",
        "    total_loss = base_loss\n",
        "\n",
        "    if is_spam is not None:\n",
        "        is_spam = torch.tensor(is_spam, device=inputs[\"input_ids\"].device).bool()\n",
        "\n",
        "        if is_spam.any():\n",
        "            if prompts is not None:\n",
        "                prompt_texts = [prompts[i] for i in range(len(prompts)) if is_spam[i]]\n",
        "                prompt_inputs = tokenizer(\n",
        "                    prompt_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                ).to(model.device)\n",
        "            else:\n",
        "                prompt_inputs = {\n",
        "                    \"input_ids\": inputs[\"input_ids\"][is_spam],\n",
        "                    \"attention_mask\": inputs[\"attention_mask\"][is_spam]\n",
        "                }\n",
        "\n",
        "            gen_outputs = model.generate(\n",
        "                **prompt_inputs,\n",
        "                max_new_tokens=64,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                temperature=0.5,\n",
        "                repetition_penalty=1.2,\n",
        "            )\n",
        "            generated_texts = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                spam_probs = [\n",
        "                    baseline_lr.predict_proba([text])[0][1]\n",
        "                    for text in generated_texts\n",
        "                ]\n",
        "                spam_scores = torch.tensor(spam_probs, device=model.device).clamp(0.0, 1.0)\n",
        "\n",
        "            def ascii_ratio(s): return sum(c in string.printable for c in s) / max(len(s), 1)\n",
        "            ascii_ratios = [ascii_ratio(t) for t in generated_texts]\n",
        "            ascii_penalty = 1.0 - torch.tensor(ascii_ratios, device=spam_scores.device).clamp(0.0, 1.0)\n",
        "            ascii_loss = ascii_penalty.mean()\n",
        "\n",
        "            # Spam cue reward\n",
        "            SPAM_CUES = [\"account\", \"delivery\", \"confirmation\", \"payment\", \"package\", \"wallet\", \"invoice\", \"update\", \"shipping\", \"tracking\"]\n",
        "            def spam_cue_score(text):\n",
        "                return sum(1 for word in SPAM_CUES if word in text.lower()) / max(len(text.split()), 1)\n",
        "            spam_cue_scores = [spam_cue_score(t) for t in generated_texts]\n",
        "            spam_reward = torch.tensor(spam_cue_scores, device=model.device).mean()\n",
        "\n",
        "            # Technical language penalty\n",
        "            TECH_CUES = [\"usage\", \"options\", \"--help\", \"cli\", \"command\", \"script\", \"smtp\", \"sendmail\", \"email headers\", \"python\", \"shell\", \"terminal\", \"port\", \"relay\"]\n",
        "            def tech_penalty_score(text):\n",
        "                return sum(1 for word in TECH_CUES if word in text.lower()) / max(len(text.split()), 1)\n",
        "            tech_scores = [tech_penalty_score(t) for t in generated_texts]\n",
        "            tech_penalty = torch.tensor(tech_scores, device=model.device).mean()\n",
        "\n",
        "            # Detector loss\n",
        "            detector_loss = (spam_scores ** 2).mean()\n",
        "\n",
        "            # Fluency loss\n",
        "            fluency_inputs = tokenizer(\n",
        "                generated_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "            ).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                fluency_outputs = model(**fluency_inputs, labels=fluency_inputs[\"input_ids\"])\n",
        "                fluency_loss = fluency_outputs.loss\n",
        "\n",
        "            # Dynamic adversarial scaling\n",
        "            mean_score = spam_scores.mean().item()\n",
        "            alpha = 1.0 + 5.0 * max(0, mean_score - 0.995)\n",
        "\n",
        "            total_loss = (\n",
        "                base_loss\n",
        "                + alpha * detector_loss\n",
        "                + 0.5 * ascii_loss\n",
        "                + 0.5 * fluency_loss\n",
        "                + 0.3 * tech_penalty\n",
        "                - 0.3 * spam_reward\n",
        "            )\n",
        "\n",
        "            if self.state.global_step % 10 == 0:\n",
        "                self.losses.append(total_loss.item())\n",
        "                self.steps.append(self.state.global_step)\n",
        "\n",
        "                clear_output(wait=True)\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.plot(self.steps, self.losses, marker='o', label=\"Training Loss\")\n",
        "                plt.xlabel(\"Step\")\n",
        "                plt.ylabel(\"Loss\")\n",
        "                plt.title(\"Live Training Loss\")\n",
        "                plt.grid(True)\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                display(plt.gcf())\n",
        "                plt.close()\n",
        "\n",
        "                self.log({\n",
        "                    \"base_loss\": base_loss.item(),\n",
        "                    \"spam_score_mean\": mean_score,\n",
        "                    \"detector_loss\": detector_loss.item(),\n",
        "                    \"ascii_penalty\": ascii_loss.item(),\n",
        "                    \"fluency_loss\": fluency_loss.item(),\n",
        "                    \"tech_penalty\": tech_penalty.item(),\n",
        "                    \"spam_reward\": spam_reward.item(),\n",
        "                    \"adversarial_weight\": alpha,\n",
        "                    \"total_loss\": total_loss.item(),\n",
        "                })\n",
        "\n",
        "                print(f\"\\n🧠 Step {self.state.global_step}\")\n",
        "                for i in range(min(2, len(generated_texts))):\n",
        "                    print(f\"[GEN {i}] {generated_texts[i][:120]}... | Spam: {spam_scores[i]:.4f} | ASCII: {ascii_ratios[i]:.2f}\")\n",
        "                print(\n",
        "                    f\"base_loss: {base_loss.item():.4f} | spam_mean: {mean_score:.4f} | \"\n",
        "                    f\"detector_loss: {detector_loss.item():.4f} | ascii_loss: {ascii_loss.item():.4f} | \"\n",
        "                    f\"fluency_loss: {fluency_loss.item():.4f} | tech_penalty: {tech_penalty.item():.4f} | \"\n",
        "                    f\"spam_reward: {spam_reward.item():.4f} | alpha: {alpha:.2f} | total_loss: {total_loss.item():.4f}\"\n",
        "                )\n",
        "\n",
        "            if mean_score > 0.999 and self.state.global_step > 100:\n",
        "                print(\"⚠️ High spam_score_mean detected — model might be gaming the detector.\")\n",
        "\n",
        "            if self.state.global_step % 100 == 0:\n",
        "                df = pd.DataFrame({\n",
        "                    \"step\": [self.state.global_step] * len(generated_texts),\n",
        "                    \"generated_text\": generated_texts,\n",
        "                    \"spam_score\": spam_scores.tolist(),\n",
        "                    \"ascii_ratio\": ascii_ratios,\n",
        "                    \"tech_penalty\": tech_scores,\n",
        "                    \"spam_reward\": spam_cue_scores\n",
        "                })\n",
        "                df.to_csv(\"generation_debug.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "    else:\n",
        "        self.log({\n",
        "            \"base_loss\": base_loss.item(),\n",
        "            \"spam_score_mean\": 0.0,\n",
        "            \"detector_loss\": 0.0,\n",
        "            \"ascii_penalty\": 0.0,\n",
        "            \"fluency_loss\": 0.0,\n",
        "            \"tech_penalty\": 0.0,\n",
        "            \"spam_reward\": 0.0,\n",
        "            \"adversarial_weight\": 0.0,\n",
        "            \"total_loss\": total_loss.item(),\n",
        "        })\n",
        "\n",
        "    return (total_loss, outputs) if return_outputs else total_loss"
      ],
      "metadata": {
        "id": "iQwkRzgPtFXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miTcK4pbtFZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "class AdversarialTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.losses = []\n",
        "        self.steps = []\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        is_spam = inputs.pop(\"is_spam\", None)\n",
        "        prompts = inputs.pop(\"prompt\", None)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        base_loss = outputs.loss\n",
        "        total_loss = base_loss\n",
        "\n",
        "        if is_spam is not None:\n",
        "            is_spam = torch.tensor(is_spam, device=inputs[\"input_ids\"].device).bool()\n",
        "\n",
        "            if is_spam.any():\n",
        "                if prompts is not None:\n",
        "                    prompt_texts = [prompts[i] for i in range(len(prompts)) if is_spam[i]]\n",
        "                    prompt_inputs = tokenizer(\n",
        "                        prompt_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                    ).to(model.device)\n",
        "                else:\n",
        "                    prompt_inputs = {\n",
        "                        \"input_ids\": inputs[\"input_ids\"][is_spam],\n",
        "                        \"attention_mask\": inputs[\"attention_mask\"][is_spam]\n",
        "                    }\n",
        "\n",
        "                gen_outputs = model.generate(\n",
        "                    **prompt_inputs,\n",
        "                    max_new_tokens=64,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    temperature=0.5,\n",
        "                    repetition_penalty=1.2,\n",
        "                )\n",
        "                generated_texts = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    spam_probs = [\n",
        "                        baseline_lr.predict_proba([text])[0][1]\n",
        "                        for text in generated_texts\n",
        "                    ]\n",
        "                    spam_scores = torch.tensor(spam_probs, device=model.device).clamp(0.0, 1.0)\n",
        "\n",
        "                def ascii_ratio(s): return sum(c in string.printable for c in s) / max(len(s), 1)\n",
        "                ascii_ratios = [ascii_ratio(t) for t in generated_texts]\n",
        "                ascii_penalty = 1.0 - torch.tensor(ascii_ratios, device=spam_scores.device).clamp(0.0, 1.0)\n",
        "                ascii_loss = ascii_penalty.mean()\n",
        "\n",
        "                detector_loss = (spam_scores ** 2).mean()\n",
        "\n",
        "                fluency_inputs = tokenizer(\n",
        "                    generated_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                ).to(model.device)\n",
        "                with torch.no_grad():\n",
        "                    fluency_outputs = model(**fluency_inputs, labels=fluency_inputs[\"input_ids\"])\n",
        "                    fluency_loss = fluency_outputs.loss\n",
        "\n",
        "                mean_score = spam_scores.mean().item()\n",
        "                alpha = 1.0 + 5.0 * max(0, mean_score - 0.995)\n",
        "\n",
        "                total_loss = base_loss + alpha * detector_loss + 0.5 * ascii_loss + 0.5 * fluency_loss\n",
        "\n",
        "                if self.state.global_step % 10 == 0:\n",
        "                    self.losses.append(total_loss.item())\n",
        "                    self.steps.append(self.state.global_step)\n",
        "\n",
        "                    # === Live loss plot ===\n",
        "                    clear_output(wait=True)\n",
        "                    plt.figure(figsize=(10, 4))\n",
        "                    plt.plot(self.steps, self.losses, marker='o', label=\"Training Loss\")\n",
        "                    plt.xlabel(\"Step\")\n",
        "                    plt.ylabel(\"Loss\")\n",
        "                    plt.title(\"Live Training Loss\")\n",
        "                    plt.grid(True)\n",
        "                    plt.legend()\n",
        "                    plt.tight_layout()\n",
        "                    display(plt.gcf())\n",
        "                    plt.close()\n",
        "\n",
        "                    self.log({\n",
        "                        \"base_loss\": base_loss.item(),\n",
        "                        \"spam_score_mean\": mean_score,\n",
        "                        \"detector_loss\": detector_loss.item(),\n",
        "                        \"ascii_penalty\": ascii_loss.item(),\n",
        "                        \"fluency_loss\": fluency_loss.item(),\n",
        "                        \"adversarial_weight\": alpha,\n",
        "                        \"total_loss\": total_loss.item(),\n",
        "                    })\n",
        "\n",
        "                    print(f\"\\n🧠 Step {self.state.global_step}\")\n",
        "                    for i in range(min(2, len(generated_texts))):\n",
        "                        print(f\"[GEN {i}] {generated_texts[i][:120]}... | Spam: {spam_scores[i]:.4f} | ASCII: {ascii_ratios[i]:.2f}\")\n",
        "                    print(\n",
        "                        f\"base_loss: {base_loss.item():.4f} | spam_mean: {mean_score:.4f} | \"\n",
        "                        f\"detector_loss: {detector_loss.item():.4f} | ascii_loss: {ascii_loss.item():.4f} | \"\n",
        "                        f\"fluency_loss: {fluency_loss.item():.4f} | alpha: {alpha:.2f} | total_loss: {total_loss.item():.4f}\"\n",
        "                    )\n",
        "\n",
        "                if mean_score > 0.999 and self.state.global_step > 100:\n",
        "                    print(\"⚠️ High spam_score_mean detected — model might be gaming the detector.\")\n",
        "\n",
        "                if self.state.global_step % 100 == 0:\n",
        "                    df = pd.DataFrame({\n",
        "                        \"step\": [self.state.global_step] * len(generated_texts),\n",
        "                        \"generated_text\": generated_texts,\n",
        "                        \"spam_score\": spam_scores.tolist(),\n",
        "                        \"ascii_ratio\": ascii_ratios,\n",
        "                    })\n",
        "                    df.to_csv(\"generation_debug.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "            else:\n",
        "                self.log({\n",
        "                    \"base_loss\": base_loss.item(),\n",
        "                    \"spam_score_mean\": 0.0,\n",
        "                    \"detector_loss\": 0.0,\n",
        "                    \"ascii_penalty\": 0.0,\n",
        "                    \"fluency_loss\": 0.0,\n",
        "                    \"adversarial_weight\": 0.0,\n",
        "                    \"total_loss\": total_loss.item(),\n",
        "                })\n",
        "\n",
        "        return (total_loss, outputs) if return_outputs else total_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ek6zBiN2ubN-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-tCMG0u8ubN-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "t101hZ7NubN-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XVVKnTo4ubN-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs_adversarial\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=500,  # You can change this\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    report_to=\"tensorboard\",  # Use \"wandb\" or \"tensorboard\" if you want visual logs\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:18:46.710744Z",
          "iopub.execute_input": "2025-04-23T10:18:46.711316Z",
          "iopub.status.idle": "2025-04-23T10:18:46.742016Z",
          "shell.execute_reply.started": "2025-04-23T10:18:46.711291Z",
          "shell.execute_reply": "2025-04-23T10:18:46.741084Z"
        },
        "id": "eO-YoBWQHzXq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bOxJz1R_HzXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adv_trainer = AdversarialTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "adv_trainer.train()\n",
        "adv_trainer.save_model(\"outputs_adversarial\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:21:08.419905Z",
          "iopub.execute_input": "2025-04-23T10:21:08.420252Z",
          "execution_failed": "2025-04-23T12:57:07.150Z"
        },
        "id": "_TaxWK3qubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the base model first\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,   # ✅ only this\n",
        "    dtype = None\n",
        ")\n",
        "\n",
        "# Step 2: Load your LoRA adapter (your checkpoint path)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/outputs_adversarial/checkpoint-100\",\n",
        "    is_trainable=False  # important to avoid accidental re-training\n",
        ")\n"
      ],
      "metadata": {
        "id": "T0TEm8ZIwGqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/outputs_adversarial/checkpoint-100\",\n",
        "    is_trainable=False  # important to avoid accidental re-training\n",
        ")"
      ],
      "metadata": {
        "id": "DHJ5DrTV09Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the spam detector\n",
        "spam_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Codes/AI Project v2/roberta-checkpoint-7660\")\n",
        "spam_detector = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Codes/AI Project v2/roberta-checkpoint-7660\")\n",
        "\n",
        "# Move to device and freeze\n",
        "spam_detector.eval()\n",
        "spam_detector.to(\"cuda\")\n",
        "for param in spam_detector.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "A-Kv6gpS02Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1chV_Ar_02P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUj3SJQR02T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DFlT1vZ02WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPpS86g502YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Zip the folder\n",
        "!zip -r outputs_adversarial.zip /content/outputs_adversarial\n",
        "\n",
        "# Download to your computer\n",
        "files.download(\"outputs_adversarial.zip\")"
      ],
      "metadata": {
        "id": "EvHahIfw3vfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i49ZkzA73viK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a spam message pretending to be a delivery notification.\"\n",
        "input_text = prompt + \"\\n\\n\"  # <--- This newline is key!\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id  # just in case\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"📨 Generated Spam Message:\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "9CsglV5F11MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Generate a fluent and believable message targeting cryptocurrency investors. \"\n",
        "    \"Avoid obvious spam words and focus on persuasive tone in natural English. \"\n",
        "    \"The message must be realistic and safe, written like a financial advisor might speak.\\n\\n\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=1.2,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id  # just in case\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(decoded)\n",
        "\n",
        "# --- 🔍 Run spam score evaluation ---\n",
        "spam_inputs = spam_tokenizer(decoded, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    spam_outputs = spam_detector(**spam_inputs)\n",
        "    probs = torch.softmax(spam_outputs.logits / 2.0, dim=-1)\n",
        "    spam_score = probs[:, 1].item()  # Confidence it's spam\n",
        "\n",
        "# --- 📊 Display spam score mean (for single example, it's the same) ---\n",
        "print(f\"\\n📈 Spam Score (RoBERTa): {spam_score:.4f}\")\n",
        "print(f\"📉 Spam Score Mean: {spam_score:.4f}\")  # Same in this case, but good for future loops\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ceEdBKQHubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare output fileimport torch\n",
        "from transformers import TextStreamer\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming the model and tokenizer are already loaded and on CUDA\n",
        "\n",
        "# Fixed prompts from user\n",
        "fixed_prompts = [\n",
        "    \"Generate a message that offers high returns from cryptocurrency investments.\",\n",
        "    \"Generate a message that convinces users to invest in a limited-time real estate opportunity.\",\n",
        "    \"Generate a message that promises to double the user’s money in 24 hours.\",\n",
        "    \"Generate a message that mimics a government financial aid offer.\",\n",
        "    \"Generate a message that appears to be from a trusted investment broker.\",\n",
        "    \"Generate a message that offers early access to IPO shares.\",\n",
        "    \"Generate a message that claims to be from a loan approval service.\",\n",
        "    \"Generate a message that promotes a fake crypto wallet app.\",\n",
        "    \"Generate a message that targets people with student loans.\",\n",
        "    \"Generate a message that offers pre-approved credit cards with no background check.\",\n",
        "    \"Generate a message that pretends to be a love interest asking for help.\",\n",
        "    \"Generate a message that lures someone with romantic intentions into clicking a link.\",\n",
        "    \"Generate a message that uses emotional manipulation to build trust.\",\n",
        "    \"Generate a message that fakes concern and asks for urgent financial help.\",\n",
        "    \"Generate a message that promises a soulmate match but requires sign-up.\",\n",
        "    \"Generate a message that offers a fake Amazon gift card.\",\n",
        "    \"Generate a message that announces a surprise Black Friday sale.\",\n",
        "    \"Generate a message that promises free samples of high-end products.\",\n",
        "    \"Generate a message that claims the user has won a mystery box.\",\n",
        "    \"Generate a message that appears to be a delivery tracking notification with a link.\",\n",
        "    \"Generate a message that claims the user's PayPal account has been locked.\",\n",
        "    \"Generate a message that says the user's Netflix subscription failed to renew.\",\n",
        "    \"Generate a message that looks like a password reset notification.\",\n",
        "    \"Generate a message that impersonates Apple support.\",\n",
        "    \"Generate a message that says suspicious activity was detected on a bank account.\",\n",
        "    \"Generate a message that promotes a miracle weight loss supplement.\",\n",
        "    \"Generate a message that offers a free trial of anti-aging pills.\",\n",
        "    \"Generate a message that promises a secret cure to chronic pain.\",\n",
        "    \"Generate a message that claims new health benefits are available for seniors.\",\n",
        "    \"Generate a message that offers fake vaccination incentives.\",\n",
        "    \"Generate a message that pretends to be a tax refund alert.\",\n",
        "    \"Generate a message that mimics a message from the social security office.\",\n",
        "    \"Generate a message that offers to expedite visa approval for a fee.\",\n",
        "    \"Generate a message that claims the user missed a court summons.\",\n",
        "    \"Generate a message that pretends to be a notice from local police.\",\n",
        "    \"Generate a message that offers a remote job with high pay.\",\n",
        "    \"Generate a message that promotes a fake internship opportunity.\",\n",
        "    \"Generate a message that encourages signing up for a grant.\",\n",
        "    \"Generate a message that promises a fast-track work visa.\",\n",
        "    \"Generate a message that looks like an HR department email.\",\n",
        "    \"Generate a message that claims the user won a free holiday trip.\",\n",
        "    \"Generate a message that offers discounted first-class flights.\",\n",
        "    \"Generate a message that offers last-minute cruise deals.\",\n",
        "    \"Generate a message that says hotel loyalty points are about to expire.\",\n",
        "    \"Generate a message that mimics an airline reward program.\",\n",
        "    \"Generate a message that urges the user to click due to urgent news.\",\n",
        "    \"Generate a message that offers financial advice from a fake influencer.\",\n",
        "    \"Generate a message that impersonates a local business offering free products.\",\n",
        "    \"Generate a message that claims the recipient's device is infected.\",\n",
        "    \"Generate a message that uses fear and urgency to provoke action.\",\n",
        "]\n",
        "\n",
        "# Group list (we'll use this to create extra prompts)\n",
        "group_list = [\n",
        "    \"Cryptocurrency investors\", \"Stock traders\", \"Real estate investors\", \"Forex traders\",\n",
        "    \"People with bad credit\", \"Loan applicants\", \"Credit card seekers\", \"Passive income seekers\",\n",
        "    \"Day traders\", \"NFT collectors\", \"Job seekers\", \"Remote work seekers\", \"Unemployed individuals\",\n",
        "    \"Recent graduates\", \"Students looking for scholarships\", \"People seeking side hustles\",\n",
        "    \"Freelancers\", \"People in debt\", \"College students needing extra income\",\n",
        "    \"People enrolled in online courses\", \"Online shoppers\", \"Amazon customers\",\n",
        "    \"People who use discount sites\", \"Holiday gift shoppers\", \"Luxury item buyers\",\n",
        "    \"Coupon users\", \"Gadget enthusiasts\", \"Online electronics shoppers\", \"Impulse buyers\",\n",
        "    \"Clothing deal seekers\", \"Dating app users\", \"Lonely individuals\", \"Divorcees\",\n",
        "    \"Widowed individuals\", \"People in long-distance relationships\", \"People seeking marriage\",\n",
        "    \"Those who post about heartbreak\", \"Older singles\", \"Young adults on dating forums\",\n",
        "    \"Individuals interested in “soulmate” content\", \"People interested in manifestation\",\n",
        "    \"Self-improvement junkies\", \"Followers of motivational speakers\", \"People seeking life coaching\",\n",
        "    \"People attending self-help webinars\", \"Entrepreneurs in mindset circles\",\n",
        "    \"Followers of hustle culture\", \"Burned-out professionals\", \"Creatives looking for purpose\",\n",
        "    \"Followers of “get rich quick” pages\", \"Weight loss seekers\", \"Supplement buyers\",\n",
        "    \"Fitness beginners\", \"Alternative medicine fans\", \"Anti-aging product buyers\",\n",
        "    \"Parents looking for baby supplements\", \"Chronic pain sufferers\", \"Mental health forum users\",\n",
        "    \"People looking for sleep hacks\", \"Keto / intermittent fasting followers\", \"Tech support seekers\",\n",
        "    \"iPhone users\", \"Android users\", \"Online gamers\", \"People who lost access to accounts\",\n",
        "    \"Email users receiving “account compromised” alerts\", \"People using VPNs\",\n",
        "    \"Users concerned about identity theft\", \"People downloading free software\", \"Torrent site users\",\n",
        "    \"Budget travellers\", \"Airline deal seekers\", \"People who use Airbnb\", \"Cruise enthusiasts\",\n",
        "    \"Backpackers\", \"Honeymoon planners\", \"People in travel Facebook groups\", \"Frequent flyers\",\n",
        "    \"People with unused travel vouchers\", \"Expats\", \"Parents with young children\", \"New homeowners\",\n",
        "    \"Retirees\", \"Dog owners\", \"Cat lovers\", \"Grandparents\", \"Recently engaged couples\",\n",
        "    \"Wedding planners\", \"Home decorators\", \"Gardening enthusiasts\", \"Tax refund claimants\",\n",
        "    \"Immigration applicants\", \"People filing unemployment benefits\", \"Veterans\",\n",
        "    \"People waiting for legal settlements\", \"Recipients of COVID-related aid\",\n",
        "    \"Voters during election season\", \"People with parking fines\", \"Jury duty no-shows\",\n",
        "    \"Students waiting for visa approvals\",\n",
        "]\n",
        "\n",
        "# Combine both into final prompt list\n",
        "final_prompts = fixed_prompts + [f\"Generate a message that targets {group.lower()}\" for group in group_list]"
      ],
      "metadata": {
        "trusted": true,
        "id": "xbk27xRKubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TextStreamer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ensure both models are on the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "spam_detector = spam_detector.to(device)\n",
        "model.eval()\n",
        "spam_detector.eval()\n",
        "\n",
        "# Container for generated data\n",
        "generated_data = []\n",
        "\n",
        "for prompt in tqdm(final_prompts, desc=\"Generating + Scoring\"):\n",
        "    input_text = prompt + \"\\n\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and strip prompt\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    completion = generated_text.replace(input_text.strip(), \"\").strip()\n",
        "\n",
        "    # Score the completion using spam_detector\n",
        "    with torch.no_grad():\n",
        "        spam_inputs = spam_tokenizer(completion, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        spam_outputs = spam_detector(**spam_inputs)\n",
        "        probs = torch.softmax(spam_outputs.logits / 2.0, dim=-1)\n",
        "        spam_score = probs[:, 1].item()  # assuming index 1 = spam\n",
        "\n",
        "    # Store result\n",
        "    generated_data.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"generated_message\": completion,\n",
        "        \"spam_score\": round(spam_score, 4)\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "df_generated = pd.DataFrame(generated_data)\n",
        "\n",
        "# Save to CSV\n",
        "df_generated.to_csv(\"generated_spam_dataset_scored.csv\", index=False)\n",
        "\n",
        "print(\"✅ Done! Dataset saved with spam scores.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MDcvwB1uubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your generated dataset with spam scores\n",
        "df = pd.read_csv(\"generated_spam_dataset_scored.csv\")\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df[\"spam_score\"], bins=30, edgecolor=\"black\", alpha=0.7)\n",
        "plt.axvline(df[\"spam_score\"].mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {df['spam_score'].mean():.4f}\")\n",
        "\n",
        "plt.title(\"Distribution of Spam Scores in Generated Dataset\")\n",
        "plt.xlabel(\"Spam Score\")\n",
        "plt.ylabel(\"Number of Messages\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "si5OblkZubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "r6s4ph30ubN_"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}