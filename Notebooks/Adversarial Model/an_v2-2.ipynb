{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11499795,
          "sourceType": "datasetVersion",
          "datasetId": 7209359
        },
        {
          "sourceId": 11499803,
          "sourceType": "datasetVersion",
          "datasetId": 7209367
        },
        {
          "sourceId": 11526880,
          "sourceType": "datasetVersion",
          "datasetId": 7229398
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Network Trial 2 - Naives Bayes Detector\n",
        "**Authors:** Matías Arévalo, Pilar Guerrero, Moritz Goebbels, Tomás Lock, Allan Stalker  \n",
        "**Date:** January – May 2025  "
      ],
      "metadata": {
        "id": "z59t1hPHubN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose\n",
        "In this notebook we use our generator checkpoint as a starting point to face a Naives Bayes detector model. Once the generator has reached an optimal point (either the number of steps is reached or has plateaud), we will use it to face our RoBERTa model. If successful, we save the best checkpoint to use in further trials."
      ],
      "metadata": {
        "id": "KELeJ6tl_jxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Download and Setup"
      ],
      "metadata": {
        "id": "iYw03kavubN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:08:20.508721Z",
          "iopub.execute_input": "2025-04-23T10:08:20.509430Z",
          "iopub.status.idle": "2025-04-23T10:09:55.115956Z",
          "shell.execute_reply.started": "2025-04-23T10:08:20.509407Z",
          "shell.execute_reply": "2025-04-23T10:09:55.114901Z"
        },
        "id": "csBSFn2DHzXm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade unsloth\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:03.175645Z",
          "iopub.execute_input": "2025-04-23T10:10:03.176722Z",
          "iopub.status.idle": "2025-04-23T10:10:17.415886Z",
          "shell.execute_reply.started": "2025-04-23T10:10:03.176694Z",
          "shell.execute_reply": "2025-04-23T10:10:17.415062Z"
        },
        "id": "wwOSU04xHzXn",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:23.642782Z",
          "iopub.execute_input": "2025-04-23T10:10:23.643111Z",
          "iopub.status.idle": "2025-04-23T10:10:27.609258Z",
          "shell.execute_reply.started": "2025-04-23T10:10:23.643073Z",
          "shell.execute_reply": "2025-04-23T10:10:27.608350Z"
        },
        "id": "Ve0_fCY3HzXo",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "from unsloth import FastLanguageModel\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import TextStreamer\n",
        "import os\n",
        "from peft import PeftModel\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "SD0HOJuOAD9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "QF4Yp7F7DKev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:10:39.205853Z",
          "iopub.execute_input": "2025-04-23T10:10:39.206409Z",
          "iopub.status.idle": "2025-04-23T10:11:33.843178Z",
          "shell.execute_reply.started": "2025-04-23T10:10:39.206388Z",
          "shell.execute_reply": "2025-04-23T10:11:33.842596Z"
        },
        "id": "FdA_DClLHzXo",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:11:38.163985Z",
          "iopub.execute_input": "2025-04-23T10:11:38.164638Z",
          "iopub.status.idle": "2025-04-23T10:11:45.223426Z",
          "shell.execute_reply.started": "2025-04-23T10:11:38.164608Z",
          "shell.execute_reply": "2025-04-23T10:11:45.222801Z"
        },
        "id": "_Osh91ISHzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Generator Checkpoint Here"
      ],
      "metadata": {
        "id": "vcGRB0bGBeU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(\n",
        "    \"../../Outputs/llama_finetuning_outputs/checkpoint-1500\"\n",
        ",\n",
        "    adapter_name=\"default\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:11:47.224564Z",
          "iopub.execute_input": "2025-04-23T10:11:47.225283Z",
          "iopub.status.idle": "2025-04-23T10:11:49.645570Z",
          "shell.execute_reply.started": "2025-04-23T10:11:47.225244Z",
          "shell.execute_reply": "2025-04-23T10:11:49.644727Z"
        },
        "id": "3Aty9uDUHzXo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load train dataset with generated data here"
      ],
      "metadata": {
        "id": "Os1PtovHBkS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('../../data/train.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:16:50.719882Z",
          "iopub.execute_input": "2025-04-23T10:16:50.720421Z",
          "iopub.status.idle": "2025-04-23T10:16:51.028462Z",
          "shell.execute_reply.started": "2025-04-23T10:16:50.720395Z",
          "shell.execute_reply": "2025-04-23T10:16:51.027838Z"
        },
        "id": "i1ZY1wNtubN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['clean_message'].dropna()\n",
        "y_train = train['label'].loc[X_train.index]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:16:54.857198Z",
          "iopub.execute_input": "2025-04-23T10:16:54.857796Z",
          "iopub.status.idle": "2025-04-23T10:16:54.868497Z",
          "shell.execute_reply.started": "2025-04-23T10:16:54.857770Z",
          "shell.execute_reply": "2025-04-23T10:16:54.867526Z"
        },
        "id": "sgEaKxxzubN9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_nb = Pipeline([\n",
        "    ('vect', CountVectorizer(\n",
        "        analyzer='word',\n",
        "        ngram_range=(1,1),\n",
        "        lowercase=True,\n",
        "        stop_words='english'\n",
        "    )),\n",
        "    ('clf', MultinomialNB(alpha=50.0))\n",
        "])\n",
        "baseline_nb.fit(X_train, y_train)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:17:10.799331Z",
          "iopub.execute_input": "2025-04-23T10:17:10.799741Z",
          "iopub.status.idle": "2025-04-23T10:17:11.699944Z",
          "shell.execute_reply.started": "2025-04-23T10:17:10.799716Z",
          "shell.execute_reply": "2025-04-23T10:17:11.699210Z"
        },
        "id": "JN_e7mNlHzXp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load full merged dataset (for ham if needed later)\n",
        "merged_df = pd.read_csv(\"../../data/merged_data.csv\")\n",
        "EOS_TOKEN = tokenizer.eos_token or \"\"\n",
        "\n",
        "# --- 🧠 Spam-Only Dataset ---\n",
        "spam_df = pd.read_csv(\"../../data/final_scam_prompt_sample.csv\")\n",
        "spam_df = spam_df.drop(columns=[\"label\"])\n",
        "spam_df.rename(columns={\"clean_message\": \"completion\"}, inplace=True)\n",
        "spam_df[\"full_text\"] = spam_df[\"prompt\"] + \"\\n\\n\" + spam_df[\"completion\"] + EOS_TOKEN\n",
        "spam_df[\"is_spam\"] = 1\n",
        "\n",
        "# ✅ Shuffle the spam dataset\n",
        "spam_df = spam_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# --- 🧠 Tokenize ---\n",
        "tokenized = tokenizer(\n",
        "    spam_df[\"full_text\"].tolist(),\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_seq_length,\n",
        "    return_tensors=\"np\",\n",
        ")\n",
        "\n",
        "# --- 📦 HuggingFace Dataset ---\n",
        "final_data = {\n",
        "    \"input_ids\": tokenized[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "    \"labels\": tokenized[\"input_ids\"].copy(),\n",
        "    \"is_spam\": spam_df[\"is_spam\"].tolist(),\n",
        "    \"prompt\": spam_df[\"prompt\"].tolist(),\n",
        "}\n",
        "dataset = Dataset.from_dict(final_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WeZv3ELzHzXp",
        "execution": {
          "iopub.status.busy": "2025-04-23T10:12:52.677661Z",
          "iopub.execute_input": "2025-04-23T10:12:52.678051Z",
          "iopub.status.idle": "2025-04-23T10:13:01.927283Z",
          "shell.execute_reply.started": "2025-04-23T10:12:52.678018Z",
          "shell.execute_reply": "2025-04-23T10:13:01.926619Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "spam_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:13:07.267541Z",
          "iopub.execute_input": "2025-04-23T10:13:07.268421Z",
          "iopub.status.idle": "2025-04-23T10:13:07.292449Z",
          "shell.execute_reply.started": "2025-04-23T10:13:07.268393Z",
          "shell.execute_reply": "2025-04-23T10:13:07.291453Z"
        },
        "id": "Pxmh6wGv665C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class AdversarialTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        is_spam = inputs.pop(\"is_spam\", None)\n",
        "        prompts = inputs.pop(\"prompt\", None)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        base_loss = outputs.loss\n",
        "        total_loss = base_loss\n",
        "\n",
        "        if is_spam is not None:\n",
        "            is_spam = torch.tensor(is_spam, device=inputs[\"input_ids\"].device).bool()\n",
        "\n",
        "            if is_spam.any():\n",
        "                # ========== Use PROMPT if available ==========\n",
        "                if prompts is not None:\n",
        "                    prompt_texts = [prompts[i] for i in range(len(prompts)) if is_spam[i]]\n",
        "                    prompt_inputs = tokenizer(\n",
        "                        prompt_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                    ).to(model.device)\n",
        "                else:\n",
        "                    prompt_inputs = {\n",
        "                        \"input_ids\": inputs[\"input_ids\"][is_spam],\n",
        "                        \"attention_mask\": inputs[\"attention_mask\"][is_spam]\n",
        "                    }\n",
        "\n",
        "                # ========== Generate spammy completions ==========\n",
        "                gen_outputs = model.generate(\n",
        "                    **prompt_inputs,\n",
        "                    max_new_tokens=64,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    temperature=0.5,\n",
        "                    repetition_penalty=1.2,\n",
        "                )\n",
        "                generated_texts = tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
        "\n",
        "                # ========== Spam score (Naive Bayes) ==========\n",
        "                with torch.no_grad():\n",
        "                    spam_probs = [\n",
        "                        baseline_nb.predict_proba([text])[0][1]\n",
        "                        for text in generated_texts\n",
        "                    ]\n",
        "                    spam_scores = torch.tensor(spam_probs, device=model.device).clamp(0.0, 1.0)\n",
        "\n",
        "                # ========== ASCII Penalty ==========\n",
        "                def ascii_ratio(s): return sum(c in string.printable for c in s) / max(len(s), 1)\n",
        "                ascii_ratios = [ascii_ratio(t) for t in generated_texts]\n",
        "                ascii_penalty = 1.0 - torch.tensor(ascii_ratios, device=spam_scores.device).clamp(0.0, 1.0)\n",
        "                ascii_loss = ascii_penalty.mean()\n",
        "\n",
        "                # ========== Spam Detection Loss ==========\n",
        "                high_conf = spam_scores > 0.95\n",
        "                detector_loss = (\n",
        "                    F.binary_cross_entropy(spam_scores[high_conf], torch.zeros_like(spam_scores[high_conf]))\n",
        "                    if high_conf.any() else torch.tensor(0.0, device=spam_scores.device)\n",
        "                )\n",
        "\n",
        "                # ========== Fluency Loss (Perplexity) ==========\n",
        "                fluency_inputs = tokenizer(\n",
        "                    generated_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                ).to(model.device)\n",
        "                with torch.no_grad():\n",
        "                    fluency_outputs = model(**fluency_inputs, labels=fluency_inputs[\"input_ids\"])\n",
        "                    fluency_loss = fluency_outputs.loss\n",
        "\n",
        "                # ========== Weighting ==========\n",
        "                mean_score = spam_scores.mean().item()\n",
        "                alpha = 1.0 + 5.0 * max(0, mean_score - 0.995)\n",
        "\n",
        "                total_loss = base_loss + alpha * detector_loss + 0.5 * ascii_loss + 0.5 * fluency_loss\n",
        "\n",
        "                # ========== Logging ==========\n",
        "                if self.state.global_step % 10 == 0:\n",
        "                    self.log({\n",
        "                        \"base_loss\": base_loss.item(),\n",
        "                        \"spam_score_mean\": mean_score,\n",
        "                        \"detector_loss\": detector_loss.item(),\n",
        "                        \"ascii_penalty\": ascii_loss.item(),\n",
        "                        \"fluency_loss\": fluency_loss.item(),\n",
        "                        \"adversarial_weight\": alpha,\n",
        "                        \"total_loss\": total_loss.item(),\n",
        "                    })\n",
        "\n",
        "                    print(f\"\\n🧠 Step {self.state.global_step}\")\n",
        "                    for i in range(min(2, len(generated_texts))):\n",
        "                        print(f\"[GEN {i}] {generated_texts[i][:120]}... | Spam: {spam_scores[i]:.4f} | ASCII: {ascii_ratios[i]:.2f}\")\n",
        "                    print(\n",
        "                        f\"base_loss: {base_loss.item():.4f} | spam_mean: {mean_score:.4f} | \"\n",
        "                        f\"detector_loss: {detector_loss.item():.4f} | ascii_loss: {ascii_loss.item():.4f} | \"\n",
        "                        f\"fluency_loss: {fluency_loss.item():.4f} | alpha: {alpha:.2f} | total_loss: {total_loss.item():.4f}\"\n",
        "                    )\n",
        "\n",
        "                if mean_score > 0.999 and self.state.global_step > 100:\n",
        "                    print(\"⚠️ High spam_score_mean detected — model might be gaming the detector.\")\n",
        "\n",
        "                if self.state.global_step % 100 == 0:\n",
        "                    df = pd.DataFrame({\n",
        "                        \"step\": [self.state.global_step] * len(generated_texts),\n",
        "                        \"generated_text\": generated_texts,\n",
        "                        \"spam_score\": spam_scores.tolist(),\n",
        "                        \"ascii_ratio\": ascii_ratios,\n",
        "                    })\n",
        "                    df.to_csv(\"generation_debug.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "            else:\n",
        "                self.log({\n",
        "                    \"base_loss\": base_loss.item(),\n",
        "                    \"spam_score_mean\": 0.0,\n",
        "                    \"detector_loss\": 0.0,\n",
        "                    \"ascii_penalty\": 0.0,\n",
        "                    \"fluency_loss\": 0.0,\n",
        "                    \"adversarial_weight\": 0.0,\n",
        "                    \"total_loss\": total_loss.item(),\n",
        "                })\n",
        "\n",
        "        return (total_loss, outputs) if return_outputs else total_loss\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:17:30.875553Z",
          "iopub.execute_input": "2025-04-23T10:17:30.876226Z",
          "iopub.status.idle": "2025-04-23T10:17:30.893050Z",
          "shell.execute_reply.started": "2025-04-23T10:17:30.876196Z",
          "shell.execute_reply": "2025-04-23T10:17:30.891998Z"
        },
        "id": "YL5KihoT665E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs_adversarial\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=250,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:18:46.710744Z",
          "iopub.execute_input": "2025-04-23T10:18:46.711316Z",
          "iopub.status.idle": "2025-04-23T10:18:46.742016Z",
          "shell.execute_reply.started": "2025-04-23T10:18:46.711291Z",
          "shell.execute_reply": "2025-04-23T10:18:46.741084Z"
        },
        "id": "eO-YoBWQHzXq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bOxJz1R_HzXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adv_trainer = AdversarialTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "adv_trainer.train()\n",
        "adv_trainer.save_model(\"outputs_adversarial\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-23T10:21:08.419905Z",
          "iopub.execute_input": "2025-04-23T10:21:08.420252Z",
          "execution_failed": "2025-04-23T12:57:07.150Z"
        },
        "id": "_TaxWK3qubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"../../Outputs/outputs_adversarial/Naive1/checkpoint-100\",\n",
        "    is_trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "T0TEm8ZIwGqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_tokenizer = AutoTokenizer.from_pretrained(\"../../Outputs/roberta_detector_outputs/roberta-checkpoint-7660\")\n",
        "spam_detector = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Codes/AI Project v2/roberta-checkpoint-7660\")\n",
        "spam_detector.eval()\n",
        "spam_detector.to(\"cuda\")\n",
        "for param in spam_detector.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "A-Kv6gpS02Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "4Xat9WF6CLq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a spam message pretending to be a delivery notification.\"\n",
        "input_text = prompt + \"\\n\\n\"\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"📨 Generated Spam Message:\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "9CsglV5F11MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Generate a fluent and believable message targeting cryptocurrency investors. \"\n",
        "    \"Avoid obvious spam words and focus on persuasive tone in natural English. \"\n",
        "    \"The message must be realistic and safe, written like a financial advisor might speak.\\n\\n\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=1.2,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(decoded)\n",
        "\n",
        "# --- 🔍 Run spam score evaluation ---\n",
        "spam_inputs = spam_tokenizer(decoded, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    spam_outputs = spam_detector(**spam_inputs)\n",
        "    probs = torch.softmax(spam_outputs.logits / 2.0, dim=-1)\n",
        "    spam_score = probs[:, 1].item()\n",
        "\n",
        "# --- 📊 Display spam score mean (for single example, it's the same) ---\n",
        "print(f\"\\n📈 Spam Score (RoBERTa): {spam_score:.4f}\")\n",
        "print(f\"📉 Spam Score Mean: {spam_score:.4f}\")  # Same in this case, but good for future loops\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ceEdBKQHubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_prompts = [\n",
        "    \"Generate a message that offers high returns from cryptocurrency investments.\",\n",
        "    \"Generate a message that convinces users to invest in a limited-time real estate opportunity.\",\n",
        "    \"Generate a message that promises to double the user’s money in 24 hours.\",\n",
        "    \"Generate a message that mimics a government financial aid offer.\",\n",
        "    \"Generate a message that appears to be from a trusted investment broker.\",\n",
        "    \"Generate a message that offers early access to IPO shares.\",\n",
        "    \"Generate a message that claims to be from a loan approval service.\",\n",
        "    \"Generate a message that promotes a fake crypto wallet app.\",\n",
        "    \"Generate a message that targets people with student loans.\",\n",
        "    \"Generate a message that offers pre-approved credit cards with no background check.\",\n",
        "    \"Generate a message that pretends to be a love interest asking for help.\",\n",
        "    \"Generate a message that lures someone with romantic intentions into clicking a link.\",\n",
        "    \"Generate a message that uses emotional manipulation to build trust.\",\n",
        "    \"Generate a message that fakes concern and asks for urgent financial help.\",\n",
        "    \"Generate a message that promises a soulmate match but requires sign-up.\",\n",
        "    \"Generate a message that offers a fake Amazon gift card.\",\n",
        "    \"Generate a message that announces a surprise Black Friday sale.\",\n",
        "    \"Generate a message that promises free samples of high-end products.\",\n",
        "    \"Generate a message that claims the user has won a mystery box.\",\n",
        "    \"Generate a message that appears to be a delivery tracking notification with a link.\",\n",
        "    \"Generate a message that claims the user's PayPal account has been locked.\",\n",
        "    \"Generate a message that says the user's Netflix subscription failed to renew.\",\n",
        "    \"Generate a message that looks like a password reset notification.\",\n",
        "    \"Generate a message that impersonates Apple support.\",\n",
        "    \"Generate a message that says suspicious activity was detected on a bank account.\",\n",
        "    \"Generate a message that promotes a miracle weight loss supplement.\",\n",
        "    \"Generate a message that offers a free trial of anti-aging pills.\",\n",
        "    \"Generate a message that promises a secret cure to chronic pain.\",\n",
        "    \"Generate a message that claims new health benefits are available for seniors.\",\n",
        "    \"Generate a message that offers fake vaccination incentives.\",\n",
        "    \"Generate a message that pretends to be a tax refund alert.\",\n",
        "    \"Generate a message that mimics a message from the social security office.\",\n",
        "    \"Generate a message that offers to expedite visa approval for a fee.\",\n",
        "    \"Generate a message that claims the user missed a court summons.\",\n",
        "    \"Generate a message that pretends to be a notice from local police.\",\n",
        "    \"Generate a message that offers a remote job with high pay.\",\n",
        "    \"Generate a message that promotes a fake internship opportunity.\",\n",
        "    \"Generate a message that encourages signing up for a grant.\",\n",
        "    \"Generate a message that promises a fast-track work visa.\",\n",
        "    \"Generate a message that looks like an HR department email.\",\n",
        "    \"Generate a message that claims the user won a free holiday trip.\",\n",
        "    \"Generate a message that offers discounted first-class flights.\",\n",
        "    \"Generate a message that offers last-minute cruise deals.\",\n",
        "    \"Generate a message that says hotel loyalty points are about to expire.\",\n",
        "    \"Generate a message that mimics an airline reward program.\",\n",
        "    \"Generate a message that urges the user to click due to urgent news.\",\n",
        "    \"Generate a message that offers financial advice from a fake influencer.\",\n",
        "    \"Generate a message that impersonates a local business offering free products.\",\n",
        "    \"Generate a message that claims the recipient's device is infected.\",\n",
        "    \"Generate a message that uses fear and urgency to provoke action.\",\n",
        "]\n",
        "\n",
        "group_list = [\n",
        "    \"Cryptocurrency investors\", \"Stock traders\", \"Real estate investors\", \"Forex traders\",\n",
        "    \"People with bad credit\", \"Loan applicants\", \"Credit card seekers\", \"Passive income seekers\",\n",
        "    \"Day traders\", \"NFT collectors\", \"Job seekers\", \"Remote work seekers\", \"Unemployed individuals\",\n",
        "    \"Recent graduates\", \"Students looking for scholarships\", \"People seeking side hustles\",\n",
        "    \"Freelancers\", \"People in debt\", \"College students needing extra income\",\n",
        "    \"People enrolled in online courses\", \"Online shoppers\", \"Amazon customers\",\n",
        "    \"People who use discount sites\", \"Holiday gift shoppers\", \"Luxury item buyers\",\n",
        "    \"Coupon users\", \"Gadget enthusiasts\", \"Online electronics shoppers\", \"Impulse buyers\",\n",
        "    \"Clothing deal seekers\", \"Dating app users\", \"Lonely individuals\", \"Divorcees\",\n",
        "    \"Widowed individuals\", \"People in long-distance relationships\", \"People seeking marriage\",\n",
        "    \"Those who post about heartbreak\", \"Older singles\", \"Young adults on dating forums\",\n",
        "    \"Individuals interested in “soulmate” content\", \"People interested in manifestation\",\n",
        "    \"Self-improvement junkies\", \"Followers of motivational speakers\", \"People seeking life coaching\",\n",
        "    \"People attending self-help webinars\", \"Entrepreneurs in mindset circles\",\n",
        "    \"Followers of hustle culture\", \"Burned-out professionals\", \"Creatives looking for purpose\",\n",
        "    \"Followers of “get rich quick” pages\", \"Weight loss seekers\", \"Supplement buyers\",\n",
        "    \"Fitness beginners\", \"Alternative medicine fans\", \"Anti-aging product buyers\",\n",
        "    \"Parents looking for baby supplements\", \"Chronic pain sufferers\", \"Mental health forum users\",\n",
        "    \"People looking for sleep hacks\", \"Keto / intermittent fasting followers\", \"Tech support seekers\",\n",
        "    \"iPhone users\", \"Android users\", \"Online gamers\", \"People who lost access to accounts\",\n",
        "    \"Email users receiving “account compromised” alerts\", \"People using VPNs\",\n",
        "    \"Users concerned about identity theft\", \"People downloading free software\", \"Torrent site users\",\n",
        "    \"Budget travellers\", \"Airline deal seekers\", \"People who use Airbnb\", \"Cruise enthusiasts\",\n",
        "    \"Backpackers\", \"Honeymoon planners\", \"People in travel Facebook groups\", \"Frequent flyers\",\n",
        "    \"People with unused travel vouchers\", \"Expats\", \"Parents with young children\", \"New homeowners\",\n",
        "    \"Retirees\", \"Dog owners\", \"Cat lovers\", \"Grandparents\", \"Recently engaged couples\",\n",
        "    \"Wedding planners\", \"Home decorators\", \"Gardening enthusiasts\", \"Tax refund claimants\",\n",
        "    \"Immigration applicants\", \"People filing unemployment benefits\", \"Veterans\",\n",
        "    \"People waiting for legal settlements\", \"Recipients of COVID-related aid\",\n",
        "    \"Voters during election season\", \"People with parking fines\", \"Jury duty no-shows\",\n",
        "    \"Students waiting for visa approvals\",\n",
        "]\n",
        "\n",
        "final_prompts = fixed_prompts + [f\"Generate a message that targets {group.lower()}\" for group in group_list]"
      ],
      "metadata": {
        "trusted": true,
        "id": "xbk27xRKubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "spam_detector = spam_detector.to(device)\n",
        "model.eval()\n",
        "spam_detector.eval()\n",
        "generated_data = []\n",
        "\n",
        "for prompt in tqdm(final_prompts, desc=\"Generating + Scoring\"):\n",
        "    input_text = prompt + \"\\n\\n\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    completion = generated_text.replace(input_text.strip(), \"\").strip()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        spam_inputs = spam_tokenizer(completion, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        spam_outputs = spam_detector(**spam_inputs)\n",
        "        probs = torch.softmax(spam_outputs.logits / 2.0, dim=-1)\n",
        "        spam_score = probs[:, 1].item()\n",
        "\n",
        "    generated_data.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"generated_message\": completion,\n",
        "        \"spam_score\": round(spam_score, 4)\n",
        "    })\n",
        "\n",
        "df_generated = pd.DataFrame(generated_data)\n",
        "df_generated.to_csv(\"../../data/generated_spam_dataset_scored.csv\", index=False)\n",
        "print(\"✅ Done! Dataset saved with spam scores.\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MDcvwB1uubN_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Inference"
      ],
      "metadata": {
        "id": "lKcqduxbB0QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../../data/generated_spam_dataset_scored.csv\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df[\"spam_score\"], bins=30, edgecolor=\"black\", alpha=0.7)\n",
        "plt.axvline(df[\"spam_score\"].mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {df['spam_score'].mean():.4f}\")\n",
        "\n",
        "plt.title(\"Distribution of Spam Scores in Generated Dataset\")\n",
        "plt.xlabel(\"Spam Score\")\n",
        "plt.ylabel(\"Number of Messages\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "si5OblkZubN_"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}