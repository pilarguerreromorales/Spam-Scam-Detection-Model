{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa Detection Model After Generated Data\n",
        "**Authors:** Matías Arévalo, Pilar Guerrero, Moritz Goebbels, Tomás Lock, Allan Stalker  \n",
        "**Date:** January – May 2025  "
      ],
      "metadata": {
        "id": "qX96TG5qKSqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose\n",
        "Create a RoBERTa Model to detect scam/spam messages. Here we use the `train.csv` and `val.csv` files we created from the generated and original data.\n",
        "\n",
        "To run this notebook, that file should be place in the `generated_data/` folder. If not, file paths should be changed in order for the notebook to run properly."
      ],
      "metadata": {
        "id": "Upwb_FzkKdVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "na8FbGaeK1dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from torch.nn import functional as F\n",
        "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "lZ7H9R78LtTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data & Preprocessing"
      ],
      "metadata": {
        "id": "x9GF1ikaMIx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "U5BUPayQMkbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('generated_data/train.csv')\n",
        "head()"
      ],
      "metadata": {
        "id": "Ga-FyUhuMI4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = pd.read_csv('generated_data/val.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8ouHAEjIMaVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['label'] = train['label'].map({'ham': 0, 'spam': 1})\n",
        "val['label'] = val['label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "id": "mLbG7Ev2OJVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X and y Values"
      ],
      "metadata": {
        "id": "Q3NxvHphRKyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train['clean_message'].dropna()\n",
        "y_train = train['label'].loc[X_train.index]\n",
        "X_val   = val['clean_message'].dropna()\n",
        "y_val   = val['label'].loc[X_val.index]"
      ],
      "metadata": {
        "id": "4fYNprCvRK3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection Model"
      ],
      "metadata": {
        "id": "x9fx8G9mMI9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "OMu9wXeJPBwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "train_encodings = tokenizer(\n",
        "    train['clean_message'].astype(str).tolist(),  # Just in case any non-strings sneak in\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val['clean_message'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=128\n",
        ")"
      ],
      "metadata": {
        "id": "IseJFQgPMJES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "This custom loss function is designed to address class imbalance in spam detection by assigning a higher weight to the minority class (spam). In standard classification tasks, models tend to focus on the majority class (ham in this case), which can lead to poor recall for the underrepresented class. By subclassing RobertaForSequenceClassification and overriding the compute_loss method, the model is explicitly instructed to penalize misclassifications of spam messages more heavily using CrossEntropyLoss with class weights (e.g., [1.0, 2.0]). This weighting scheme tells the model that predicting spam incorrectly is twice as costly as predicting ham incorrectly, encouraging it to pay more attention to spam patterns and improving its recall and F1-score for that class during training."
      ],
      "metadata": {
        "id": "qO47HKNyJ60q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0]).to(logits.device))  # weight spam more\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "EZJb2ff3J66s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {
        "id": "1EfsiI7ZKEmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = RobertaConfig.from_pretrained(\"roberta-base\", num_labels=2, hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2)\n",
        "model = CustomRobertaForSequenceClassification(config)"
      ],
      "metadata": {
        "id": "hHVAaBUqKM4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision_all, recall_all, f1_all, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    precision_spam = precision_all[1]\n",
        "    recall_spam = recall_all[1]\n",
        "    f1_spam = f1_all[1]\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    try:\n",
        "        if logits.shape[1] == 2:\n",
        "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "            auc = roc_auc_score(labels, probs[:, 1])\n",
        "        else:\n",
        "            probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "            auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
        "    except:\n",
        "        auc = float('nan')\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision_spam': precision_spam,\n",
        "        'recall_spam': recall_spam,\n",
        "        'f1_spam': f1_spam,\n",
        "        'auc': auc\n",
        "    }"
      ],
      "metadata": {
        "id": "D0tQi3_RKErn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "F9T-svRPMJMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SMSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SMSDataset(train_encodings, train['label'].tolist())\n",
        "val_dataset = SMSDataset(val_encodings, val['label'].tolist())"
      ],
      "metadata": {
        "id": "oX4dkdbMMJSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_roberta\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_spam\",  # <-- CHANGED\n",
        "    greater_is_better=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=2e-5,                    # <-- LOWER LR\n",
        "    max_grad_norm=1.0,                    # <-- GRADIENT CLIP\n",
        "    logging_dir=\"./logs_roberta\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"tensorboard\",\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "Pzg__g3vKfPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = CustomRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2),\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# deafault loss used is cross entropy\n",
        "# maybe later plug in  FocalLoss or Label Smoothing"
      ],
      "metadata": {
        "id": "YbuGLAauKfSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Y6FWOOraKtJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"results_roberta\")"
      ],
      "metadata": {
        "id": "dGuBNIxOK-HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls ./logs_roberta"
      ],
      "metadata": {
        "id": "QFh-7OexK-JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "9X8XtbvOK-Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir ./logs_roberta"
      ],
      "metadata": {
        "id": "_wTsGsaaK-Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model (Inference and Metrics)"
      ],
      "metadata": {
        "id": "NadFlX5RPPZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_dataset, batch_size=32, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluates a transformer model on a torch Dataset (like SMSDataset) and prints formatted metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1]  # Prob for class \"spam\"\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    # Convert to arrays\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = (all_probs >= threshold).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds)\n",
        "    rec = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "    pr_auc = average_precision_score(all_labels, all_probs)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\\n\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"ham\", \"spam\"]))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"ham\", \"spam\"], yticklabels=[\"ham\", \"spam\"])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return all metrics in case you want to log or save them\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"roc_auc\": auc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"confusion_matrix\": cm\n",
        "    }"
      ],
      "metadata": {
        "id": "g4X7fh1MLHg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "checkpoint_path = \"./results_roberta/checkpoint-7660\""
      ],
      "metadata": {
        "id": "BdPBFJUKLHiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(\n",
        "    model = CustomRobertaForSequenceClassification.from_pretrained(checkpoint_path),\n",
        "    val_dataset=val_dataset,\n",
        "    batch_size=32,\n",
        "    threshold=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "d2i3EK37LHlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/checkpoint-7660', 'zip', '/content/results_roberta/checkpoint-7660')"
      ],
      "metadata": {
        "id": "QCa_9YjQLHnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}