{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "collapsed_sections": [
        "seROJtXwe_rD",
        "9BcJb3D3hQc9",
        "bZBOnugLiwdf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Llama Model for Generation\n",
        "\n",
        "**Authors:** Matías Arévalo, Pilar Guerrero, Moritz Goebbels, Tomás Lock, Allan Stalker  \n",
        "**Date:** January – May 2025  "
      ],
      "metadata": {
        "id": "GCXmTnWGK69x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose\n",
        "Fine-tunes a LLaMA based language model using LoRA adapters to generate spam messages based on the preprocessed dataset with generated prompts."
      ],
      "metadata": {
        "id": "cRJbqTh4L96v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONSIDERATION\n",
        "Due to some updates done to the `unsloth` package in early April, some outputs might differ from the ones used for our model. Because of this, the `output` used from this code in the following notebooks is going to be provided in the `Outputs` folder in the repository, in the subfolder called `fine_tuned_llama` as `checkpoint_1500` so it can be used for replication.  \n",
        "In case one runs the notebook again, some differences will be found."
      ],
      "metadata": {
        "id": "t0hX10EVawMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "ytYzTFBZNYfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ],
      "metadata": {
        "id": "ODArsH-NNcyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade unsloth\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "rSZOU3ZNN3XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "ofkfB7AiMB71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlsisUHHK4g8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup  \n",
        "In this section, we configure the key parameters for loading the LLaMA-based mode we will be fine tuning later on the notebook.\n",
        "\n",
        "We set the following parameters:  \n",
        "- `max_seq_length` : used to define the maximum number of tokens the model will take as input. For efficient training we set it to `2048`, however it is important to note that LLaMA-3 models can handle up to 8k tokens.\n",
        "- `dtype` : used to specify the data types for the models weights. To automatically select the appropriate type based on the hardware we set it to `none`.\n",
        "- `load_in_4bit` : used to enable 4-bit optimization, which significantly reduces memory usage. As it is good when fine-tuning models on limited-resource environments, we set it to `True`.\n",
        "\n",
        "\n",
        "Additionally, we included a list of a set of available 4-bit fine tuned models from the Unsloth library that can be used if different variants are needed throughout the notebook."
      ],
      "metadata": {
        "id": "seROJtXwe_rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "]"
      ],
      "metadata": {
        "id": "cRyIJsdqe_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Model  \n",
        "In this section we load the pre-trained LLaMA-3 8B model using the Unsloth library, applying the previously defined settings for sequence length, data type, and quantization.\n",
        "\n",
        "For this we use the `FastLanguageModel.from_pretrained()` function to initialize bothe the model and tokenizer."
      ],
      "metadata": {
        "id": "9BcJb3D3hQc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "HHcix-6shQiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning the Model  \n",
        "Throughout this section, the different steps to fine tune the loaded model will be made."
      ],
      "metadata": {
        "id": "bZBOnugLiwdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning Preparation  \n",
        "Here we load and preprocess the dataset used for fine-tuning.\n",
        "\n",
        "The file to use is called `final_scam_prompt_dataset.csv`, which should be located in the `data/` folder."
      ],
      "metadata": {
        "id": "R_Lh_-gBi-8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Load the dataset and preview"
      ],
      "metadata": {
        "id": "NR_tFScCmVyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../../data/final_scam_prompt_dataset.csv\")"
      ],
      "metadata": {
        "id": "TWtJ7j_wmV9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"final_scam_prompt_dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hqgL7zCkmmPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Drop the `label` column.\n",
        "\n",
        "We drop this column because for two reasons:  \n",
        "- Every entry in this dataset is `spam`  \n",
        "- It is not going to be used in this notebook"
      ],
      "metadata": {
        "id": "RvmIONUCmWJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"label\"])"
      ],
      "metadata": {
        "id": "WqP4uQV4mWPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Rename the `message` column"
      ],
      "metadata": {
        "id": "oD6rXQkcmWUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={\"clean_message\": \"completion\"}, inplace=True)"
      ],
      "metadata": {
        "id": "ymvIhu2pmWaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: Merge `prompt` and `completions`"
      ],
      "metadata": {
        "id": "3dayJXxyn7lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token or \"\"\n",
        "def combine_prompt(row):\n",
        "    return row[\"prompt\"] + \"\\n\\n\" + row[\"completion\"] + EOS_TOKEN\n",
        "df[\"full_text\"] = df.apply(combine_prompt, axis=1)"
      ],
      "metadata": {
        "id": "qV0diyHSoT6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**: Tokenize input texts"
      ],
      "metadata": {
        "id": "5s6Ku0kooUS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised = tokenizer(\n",
        "    df[\"full_text\"].tolist(),\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_seq_length,\n",
        "    return_tensors=\"np\",\n",
        ")"
      ],
      "metadata": {
        "id": "kv2tZB3joUgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6**: Build final dataset"
      ],
      "metadata": {
        "id": "bAPgGyTBotvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = {\n",
        "    \"input_ids\": tokenised[\"input_ids\"],\n",
        "    \"attention_mask\": tokenised[\"attention_mask\"],\n",
        "    \"labels\": tokenised[\"input_ids\"].copy()\n",
        "}\n",
        "dataset = Dataset.from_dict(final_data)"
      ],
      "metadata": {
        "id": "rFih-gFyi_DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply LoRA Adapters  \n",
        "Here we apply lightweight LoRA adapters to the base model, allowing parameter efficient fine tuning."
      ],
      "metadata": {
        "id": "FZpYhVkU39e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "ul8r97n339l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training Configuration  \n",
        "Here we set the different characteristics and parameters needed for the model we are making to be trained on."
      ],
      "metadata": {
        "id": "a8-Oh9tX9B6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"llama_finetuning_outputs\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    max_steps=1500,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        ")"
      ],
      "metadata": {
        "id": "UhYLRV25B9HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Trainer"
      ],
      "metadata": {
        "id": "AnmtCHQCB9MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "ioUfwNuZB9Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Save the Model  \n",
        "We are going to save the model as `llama_finetuning_outputs` within the `Outputs`folder, which is at the same level as the `data` folder.\n",
        "\n",
        "In the case of this repository, we will include the last checkpoint of this training model in the `Outputs` folder within the `llama_finetuning_outputs` subfolder. This checkpoint, saved as `checkpoint-1500`, is the one we are going to use in further notebooks and can be directly loaded into them without needed to run the following training process."
      ],
      "metadata": {
        "id": "-XN4NDtzB9We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'\n",
        "trainer.train()\n",
        "trainer.save_model(\"../../Outputs/llama_finetuning_outputs\")"
      ],
      "metadata": {
        "id": "NfiJ1FZyB9bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Fine Tuned Model\n",
        "In this section, we will set up the environment following the previous setup made and load the `checkpoint-1500` found within the `llama_finetuning_outputs` folder, which should be located within the `Outputs` folder to apply it."
      ],
      "metadata": {
        "id": "yIlKHJtqNDkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Base Model (Same as the one for Training)"
      ],
      "metadata": {
        "id": "h9ozDm5KN9YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "XNJsk2_hN9fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply LoRA Adapters (Same as the one for Training)"
      ],
      "metadata": {
        "id": "suvqWZDrN9lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "eiEi-dmzN9rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fine Tuned Weights\n",
        "This is the output we saved during the training of the model, it should be:  \n",
        "- Called `llama_finetuning_outputs`\n",
        "- Located in `Outputs` folder\n",
        "\n",
        "However, for this example we are going to be using the checkpoint we have manually loaded into the `Outputs` folder, the location and file name is:\n",
        "- File name: `checkpoint-1500`\n",
        "- Location: `Outputs/llama_finetuning_outputs`"
      ],
      "metadata": {
        "id": "-hxzTzkROPtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(\"../../Outputs/llama_finetuning_outputs/checkpoint-1500\", adapter_name=\"default\")"
      ],
      "metadata": {
        "id": "YmXE4eRINDrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Fine Tuned Model: Generate Sample Outputs\n",
        "\n",
        "Use the fine tuned model to generate spam messages based on custom prompts to see how it generates messages."
      ],
      "metadata": {
        "id": "B7hbc9JLQqTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 1: 150 Maximum New Tokens"
      ],
      "metadata": {
        "id": "lkkIosX1RKvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "prompt = \"Generate a spam message targeting people interested in cryptocurrency investments.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")"
      ],
      "metadata": {
        "id": "OIpQd0iORNRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test 2: 200 Maximum New Tokens"
      ],
      "metadata": {
        "id": "0bCKWfi_RNX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a spam message targeting individuals interested in investment opportunities, including cryptocurrency, stocks, and forex, with an emphasis on high returns.\\n\\nMessage:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")"
      ],
      "metadata": {
        "id": "vAt24xzJRNds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}